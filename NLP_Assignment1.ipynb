{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rishisai0811/NLP_ASSIGNMENT/blob/main/NLP_Assignment1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "EwfYov8zUp8t"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Function to tokenize the text into words\n",
        "def tokenize_text(text):\n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    return tokens\n",
        "\n",
        "# Function to generate unigram, bigram, and trigram models\n",
        "def generate_ngram_models(text):\n",
        "    tokens = tokenize_text(text)\n",
        "    unigram_model = Counter(tokens)\n",
        "    bigram_model = Counter(bigrams(tokens))\n",
        "    trigram_model = Counter(trigrams(tokens))\n",
        "\n",
        "    # Calculate probabilities for each model\n",
        "    total_unigrams = sum(unigram_model.values())\n",
        "    total_bigrams = sum(bigram_model.values())\n",
        "    total_trigrams = sum(trigram_model.values())\n",
        "\n",
        "    unigram_probabilities = {word: count / total_unigrams for word, count in unigram_model.items()}\n",
        "    bigram_probabilities = {(word1, word2): count / unigram_model[word1] for (word1, word2), count in bigram_model.items()}\n",
        "    trigram_probabilities = {(word1, word2, word3): count / bigram_model[(word1, word2)] for (word1, word2, word3), count in trigram_model.items()}\n",
        "\n",
        "    return unigram_probabilities, bigram_probabilities, trigram_probabilities\n",
        "\n",
        "# Sample usage\n",
        "with open('/content/Fahrenheit 451 Full Text .txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "unigram_probs, bigram_probs, trigram_probs = generate_ngram_models(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Function to generate text using n-gram models\n",
        "def generate_text(seed_word, unigram_probs, bigram_probs, trigram_probs, num_sentences=5):\n",
        "    current_word = seed_word\n",
        "    generated_text = [current_word.capitalize()]\n",
        "\n",
        "    while len(generated_text) < num_sentences:\n",
        "        next_word = None\n",
        "\n",
        "        # Choose next word based on models\n",
        "        if (current_word,) in unigram_probs:\n",
        "            next_word = random.choices(list(unigram_probs.keys()), weights=list(unigram_probs.values()))[0]\n",
        "        elif (current_word,) in bigram_probs:\n",
        "            next_word = random.choices(list(bigram_probs[(current_word,)].keys()), weights=list(bigram_probs[(current_word,)].values()))[0]\n",
        "        elif (current_word,) in trigram_probs:\n",
        "            next_word = random.choices(list(trigram_probs[(current_word,)].keys()), weights=list(trigram_probs[(current_word,)].values()))[0]\n",
        "\n",
        "        # Handle the case when next_word is None\n",
        "        if next_word is None:\n",
        "            next_word = random.choices(list(unigram_probs.keys()), weights=list(unigram_probs.values()))[0]\n",
        "\n",
        "        # Update current word for the next iteration\n",
        "        current_word = next_word\n",
        "        generated_text.append(current_word)\n",
        "\n",
        "        # If a sentence-ending punctuation is encountered, start a new sentence\n",
        "        if current_word.endswith(('.', '!', '?')):\n",
        "            generated_text[-1] = generated_text[-1][:-1]  # Remove the punctuation from the last word\n",
        "            generated_text.append(random.choice(['', '', 'However', 'Meanwhile', 'In addition', 'Moreover']).capitalize())  # Choose a new sentence starter\n",
        "\n",
        "    return ' '.join(generated_text) + '.'\n",
        "\n",
        "# Sample usage\n",
        "seed_word = 'The'\n",
        "generated_paragraph = generate_text(seed_word, unigram_probs, bigram_probs, trigram_probs, num_sentences=100)\n",
        "print(generated_paragraph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnfZ8VmBjMf-",
        "outputId": "e987a8bf-b0d2-40ab-fcf3-c5540895f623"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The ear need butter stopped close what his must fill illuminated word might to of looked confuse of help furnace men thing t s furled t crackle up from ago bowling vessel did it the i in turn of t the pages sat don life foot the as hear yammering again notions in her game hand me now it he he out of as face they the sat lonely alone performed the my world combination you go as them s steadily with have if burning belief bones once to like stretched so faced the city the rushed chinese with mosquito.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Function to calculate perplexity for a given model\n",
        "def calculate_perplexity(model_probs, tokens):\n",
        "    entropy = 0\n",
        "    num_tokens = len(tokens)\n",
        "\n",
        "    for ngram in model_probs:\n",
        "        if ngram in model_probs:\n",
        "            prob = model_probs[ngram]\n",
        "        else:\n",
        "            prob = 1 / num_tokens  # If unseen n-gram, assign a small probability\n",
        "\n",
        "        entropy += -math.log(prob, 2)\n",
        "\n",
        "    perplexity = 2 ** (entropy / num_tokens)\n",
        "    return perplexity\n",
        "\n",
        "# Sample usage\n",
        "tokens = tokenize_text(text)\n",
        "unigram_perplexity = calculate_perplexity(unigram_probs, tokens)\n",
        "bigram_perplexity = calculate_perplexity(bigram_probs, tokens)\n",
        "trigram_perplexity = calculate_perplexity(trigram_probs, tokens)\n",
        "\n",
        "print(f'Unigram Perplexity: {unigram_perplexity}')\n",
        "print(f'Bigram Perplexity: {bigram_perplexity}')\n",
        "print(f'Trigram Perplexity: {trigram_perplexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ehcn0DMqjxOB",
        "outputId": "5562c22d-7fc0-4762-b687-b01ce210eb83"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigram Perplexity: 2.852767714448319\n",
            "Bigram Perplexity: 7.183178993170789\n",
            "Trigram Perplexity: 2.195084103640733\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add Laplace smoothing to n-gram models\n",
        "def add_laplace_smoothing(ngram_model, tokens, alpha=1):\n",
        "    vocabulary_size = len(set(tokens))\n",
        "    total_ngrams = len(tokens) - len(ngram_model) + 1\n",
        "\n",
        "    smoothed_model = defaultdict(lambda: 1 / (total_ngrams + alpha * vocabulary_size))\n",
        "    for ngram in ngram_model:\n",
        "        smoothed_model[ngram] = (ngram_model[ngram] + alpha) / (tokens.count(ngram[0]) + alpha * vocabulary_size)\n",
        "\n",
        "    return smoothed_model\n",
        "\n",
        "# Applying Laplace smoothing to models\n",
        "smoothed_unigram_probs = add_laplace_smoothing(unigram_model, tokens)\n",
        "smoothed_bigram_probs = add_laplace_smoothing(bigram_model, tokens)\n",
        "smoothed_trigram_probs = add_laplace_smoothing(trigram_model, tokens)\n",
        "\n",
        "# Calculating perplexity again with smoothed models\n",
        "smoothed_unigram_perplexity = calculate_perplexity(smoothed_unigram_probs, tokens)\n",
        "smoothed_bigram_perplexity = calculate_perplexity(smoothed_bigram_probs, tokens)\n",
        "smoothed_trigram_perplexity = calculate_perplexity(smoothed_trigram_probs, tokens)\n",
        "\n",
        "print(f'Smoothed Unigram Perplexity: {smoothed_unigram_perplexity}')\n",
        "print(f'Smoothed Bigram Perplexity: {smoothed_bigram_perplexity}')\n",
        "print(f'Smoothed Trigram Perplexity: {smoothed_trigram_perplexity}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOxfKgu2j5qR",
        "outputId": "093ae1d2-80c9-491f-c39b-551f641d9bb6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smoothed Unigram Perplexity: 1.0005210347833324\n",
            "Smoothed Bigram Perplexity: 1.000342354483388\n",
            "Smoothed Trigram Perplexity: 1.0001650918186362\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMO8SL/32M8gb+gfko3oUt/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}